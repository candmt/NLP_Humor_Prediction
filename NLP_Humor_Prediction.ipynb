
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_cw_script.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QmN2KjgFzQTq",
        "TBApoO9UphP2",
        "12ChvwaspnIk",
        "-dLv2L1npu-W",
        "Z6eqY5syp3uE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "78eff565636d4a04ab3547212af44ee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_33f1e506e8e74f3296ca571676ff34c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_863683ff1b74408bb81af2c88d5d1966",
              "IPY_MODEL_05cc00f9d5ee4508b09d3f2e190dfb49"
            ]
          }
        },
        "33f1e506e8e74f3296ca571676ff34c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "863683ff1b74408bb81af2c88d5d1966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8deef66686874f929d151bbf41c1c3a8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fd20b5c224f34deda1b6dbe8abf63030"
          }
        },
        "05cc00f9d5ee4508b09d3f2e190dfb49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b333b126c56b4c0fa4fdc4c5ecb5f483",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 3.35MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3bded39a5d943e48a3bac767630bbec"
          }
        },
        "8deef66686874f929d151bbf41c1c3a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fd20b5c224f34deda1b6dbe8abf63030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b333b126c56b4c0fa4fdc4c5ecb5f483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3bded39a5d943e48a3bac767630bbec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "959d7bf9ba684e60a28386e0cef4aa67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_15dd2b6c2dc9487a8db956f452b09eca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_42ab2f07c2b24277bae1bbc480fd9f70",
              "IPY_MODEL_d470c55363cd47c08978504c1a5d7e38"
            ]
          }
        },
        "15dd2b6c2dc9487a8db956f452b09eca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42ab2f07c2b24277bae1bbc480fd9f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ac6c57094aa74a0b87096d687fd9e6ea",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13c41af5b1c24c23afa8e1ca3cfe9bb5"
          }
        },
        "d470c55363cd47c08978504c1a5d7e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b02f95372366490db735caabeb963455",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 4.50kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f32933a7475b4a6fba425da4112b027b"
          }
        },
        "ac6c57094aa74a0b87096d687fd9e6ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13c41af5b1c24c23afa8e1ca3cfe9bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b02f95372366490db735caabeb963455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f32933a7475b4a6fba425da4112b027b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a08183136d0b422a84586cae4bb5079e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5db217c95dce4a8d800712f8744586e4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7138d66a0b6b4205b448bac62e1eb308",
              "IPY_MODEL_a2cbbbb69ddb43559cbbcde35b2c86bb"
            ]
          }
        },
        "5db217c95dce4a8d800712f8744586e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7138d66a0b6b4205b448bac62e1eb308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_26c7a4ad490848d0bedd960daf507ca6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f07b59b16a74008912259901526f276"
          }
        },
        "a2cbbbb69ddb43559cbbcde35b2c86bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cf1ea88fe6db47ed93c15f01f78607de",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:07&lt;00:00, 55.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d145ba4906414780855875e2b8c85fab"
          }
        },
        "26c7a4ad490848d0bedd960daf507ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f07b59b16a74008912259901526f276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cf1ea88fe6db47ed93c15f01f78607de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d145ba4906414780855875e2b8c85fab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rJL36t6IzXc"
      },
      "source": [
        "# NLP Coursework 2021: Assessing the Funniness of Edited News Headlines\n",
        "\n",
        "Monika Jotautaite, Anna Hledikova, Candela Martinez Mirat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoBS78nBAg2C"
      },
      "source": [
        "### README\n",
        "\n",
        "This notebook contains code for Task 2 of the Codalab competition called *Assessing the Funniness of Edited News Headlines*. \\\\\n",
        "\n",
        "The code for both Approach 1 and Approach 2 of the coursework is included and the notebook can be run as is. \n",
        "However, please note that some cells take a while to run, so we recommend skimming through the saved outputs first  (e.g. for hyperparamater search results).\n",
        "\n",
        "Approach 1 contains 6 version of the BERT model, with hyperparameter search done for the second version, which showed the most promise.\n",
        "\n",
        "Approach 2 contains our experiments described in section 4 of the report, namely applying a FFNN to evaluate 'funniness' of individual words and a CBOW for headline classification based on the compatibility of the original headline and the edit word.\n",
        "For more details on the notebook structure please see the table of contents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8YXZcfkI-ta"
      },
      "source": [
        "### Initial set-up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6cbiSg7KIlh"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxFXDcpRbDJq"
      },
      "source": [
        "# Imports\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import torch.optim as optim\n",
        "import codecs\n",
        "import tqdm\n",
        "\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW\n",
        "import re\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY3_g3HobGby"
      },
      "source": [
        "# Setting random seed and device\n",
        "\n",
        "def seed(value=1):\n",
        "    \"\"\"\n",
        "    We set a random seed for better reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(value)\n",
        "    np.random.seed(value)\n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed_all(value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed()\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc4A7vLoDORN"
      },
      "source": [
        "# get the data from google drive\n",
        "\n",
        "!wget -O test.csv https://drive.google.com/u/0/uc?id=1QCY3SPdVj5QMygL0mIZlv8vZ3PMRIcs4&export=download\n",
        "!wget -O dev.csv https://drive.google.com/u/0/uc?id=19B26WPcwh0USNcflb9ab0AVmg88LdCNS&export=download\n",
        "!wget -O train_funlines.csv https://drive.google.com/u/0/uc?id=1FAquNgmICfCuMZRefSB0GNGKnDSskDIh&export=download\n",
        "!wget -O train.csv https://drive.google.com/u/0/uc?id=1sx15OVfmoEelmsNHlY4JtspLjHGRN1f6&export=download\n",
        "!wget -O healines_df.csv https://drive.google.com/u/0/uc?id=1XrACKBhj3xztL63-RZOD84QpRPjAjC8k&export=download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0HotOZrxjN_"
      },
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "dev_df = pd.read_csv('dev.csv')\n",
        "\n",
        "# Load additional training data\n",
        "train_extra_df = pd.read_csv('train_funlines.csv')\n",
        "\n",
        "extra_headlines = pd.read_csv('healines_df.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9bqVOulKpC4"
      },
      "source": [
        "## Approach 1: Pre-trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9kJjuHMJc4_"
      },
      "source": [
        "The following functions help prepare our data corpus and labels for train, dev and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEb98I51MPDO"
      },
      "source": [
        "# Set hyperparameters\n",
        "\n",
        "epochs = 4 # BERT authors recommend 2-4\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XirKXVDKbKqb"
      },
      "source": [
        "def get_orig_headl_and_new_word_tuples(data):\n",
        "    \"\"\"\n",
        "    Takes a pandas data frame as input.\n",
        "    Each sample of the data set contains a headline article, two edited\n",
        "    versions of this article and a label indicating which of the two edits is\n",
        "    funnier.\n",
        "    Selects relevant columns of the input data, one with the original headlines\n",
        "    and one with the new word to be inserted instead of one of its words, and \n",
        "    converts them into lists of tuples.\n",
        "    \"\"\"\n",
        "\n",
        "    headlines_to_edit_1 = [(original_1, new_word_1) for (original_1, new_word_1) \\\n",
        "                           in zip(data.original1.to_list(), data.edit1.to_list())]\n",
        "\n",
        "    headlines_to_edit_2 = [(original_2, new_word_2) for (original_2, new_word_2) \\\n",
        "                           in zip(data.original2.to_list(), data.edit2.to_list())]\n",
        "\n",
        "    labels = data.label.to_list()\n",
        "\n",
        "    return headlines_to_edit_1, headlines_to_edit_2, labels\n",
        "\n",
        "\n",
        "def get_edited_headlines(headline_tuples:list)-> list:\n",
        "    \"\"\"\n",
        "    Takes a list of tuples of form (original_headline, new_word) as input.\n",
        "    Returns a list of edited headlines.\n",
        "    \"\"\"\n",
        "    # list of new edited headlines\n",
        "    edited_headlines = []\n",
        "\n",
        "    # The word to be replaced in each sentence is denoted as follows:\n",
        "    # <word/> to be replaced\n",
        "    pattern = re.compile(r'\\<(.*?)\\/\\>')\n",
        "    \n",
        "    for original, new_word in headline_tuples:\n",
        "      edited_headline = pattern.sub(new_word, original)\n",
        "      edited_headlines.append(edited_headline)\n",
        "\n",
        "    return edited_headlines\n",
        "\n",
        "\n",
        "def get_original_headlines(data):\n",
        "    \"\"\"\n",
        "    Takes a pandas data frame as input.\n",
        "    Returns a list of the original headlines without brackets around the word\n",
        "    to be replaced.\n",
        "    \"\"\"\n",
        "\n",
        "    original_headlines = []\n",
        "    pattern = re.compile(r'\\<(.*?)\\/\\>')\n",
        "\n",
        "    for headline in data.original1.to_list():\n",
        "\n",
        "        # finds the word to be replaced\n",
        "        origin_word = re.search('\\<(.*?)\\/\\>', headline)\n",
        "\n",
        "        #removes the <, > and / symbols around the word\n",
        "        origin_word = re.sub('[<>/]', '', origin_word.group())\n",
        "\n",
        "        orig_headline = pattern.sub(origin_word, headline)\n",
        "        original_headlines.append(orig_headline)\n",
        "    \n",
        "    return original_headlines\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv7v5oYwbNnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51407beb-752b-4fdf-d87d-4b067a31a6b5"
      },
      "source": [
        "### process training data\n",
        "\n",
        "orig_train_headlines = get_original_headlines(train_df)\n",
        "h_to_edit_1, h_to_edit_2, train_labels = get_orig_headl_and_new_word_tuples(train_df)\n",
        "\n",
        "edited_headlines_1 = get_edited_headlines(h_to_edit_1)\n",
        "edited_headlines_2 = get_edited_headlines(h_to_edit_2)\n",
        "\n",
        "edited_headlines_1_2 = [tup for tup in zip(edited_headlines_1, edited_headlines_2)]\n",
        "\n",
        "print('Original headline:', h_to_edit_1[0][0])\n",
        "print('Edited headline:', edited_headlines_1[0])\n",
        "\n",
        "### process dev data\n",
        "\n",
        "orig_dev_headlines = get_original_headlines(dev_df)\n",
        "dev_h_to_edit_1, dev_h_to_edit_2, dev_labels = get_orig_headl_and_new_word_tuples(dev_df)\n",
        "\n",
        "dev_edited_headlines_1 = get_edited_headlines(dev_h_to_edit_1)\n",
        "dev_edited_headlines_2 = get_edited_headlines(dev_h_to_edit_2)\n",
        "\n",
        "### process extended training data\n",
        "\n",
        "ext_train_df = pd.concat([train_df, train_extra_df])\n",
        "\n",
        "orig_ext_headlines = get_original_headlines(ext_train_df)\n",
        "ext_h_to_edit_1, ext_h_to_edit_2, ext_labels = get_orig_headl_and_new_word_tuples(ext_train_df)\n",
        "\n",
        "ext_edited_headlines_1 = get_edited_headlines(ext_h_to_edit_1)\n",
        "ext_edited_headlines_2 = get_edited_headlines(ext_h_to_edit_2)\n",
        "\n",
        "### process test data\n",
        "\n",
        "orig_test_headlines = get_original_headlines(test_df)\n",
        "t_h_to_edit_1, t_h_to_edit_2, test_labels = get_orig_headl_and_new_word_tuples(test_df)\n",
        "                                                                            \n",
        "test_edited_headlines_1 = get_edited_headlines(t_h_to_edit_1)\n",
        "test_edited_headlines_2 = get_edited_headlines(t_h_to_edit_2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original headline: \" Gene Cernan , Last <Astronaut/> on the Moon , Dies at 82 \"\n",
            "Edited headline: \" Gene Cernan , Last Dancer on the Moon , Dies at 82 \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSn3wlrDpSIA"
      },
      "source": [
        "Below, we tokenize our corpuses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug3OI9OFbcpq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "78eff565636d4a04ab3547212af44ee5",
            "33f1e506e8e74f3296ca571676ff34c0",
            "863683ff1b74408bb81af2c88d5d1966",
            "05cc00f9d5ee4508b09d3f2e190dfb49",
            "8deef66686874f929d151bbf41c1c3a8",
            "fd20b5c224f34deda1b6dbe8abf63030",
            "b333b126c56b4c0fa4fdc4c5ecb5f483",
            "e3bded39a5d943e48a3bac767630bbec"
          ]
        },
        "outputId": "463c03f5-d453-48a4-e971-88880ca79ea7"
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "\n",
        "def find_max_len(sentences, n_components):\n",
        "    max_len = 0\n",
        "    for sentence in sentences:\n",
        "        max_len = max(max_len, len(sentence.split()))\n",
        "    max_len *= n_components\n",
        "    max_len += n_components\n",
        "    return max_len\n",
        "\n",
        "# get the maximum input length for padding purposes\n",
        "max_h_len = max(find_max_len(edited_headlines_1, 2), \n",
        "                find_max_len(ext_edited_headlines_1,2))\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "### train set\n",
        "\n",
        "train_set_embedding = tokenizer(edited_headlines_1, edited_headlines_2, \n",
        "                            max_length = max_h_len, padding = 'max_length',\n",
        "                            truncation = True, return_tensors =\"pt\")\n",
        "\n",
        "###dev set\n",
        "\n",
        "dev_set_embedding = tokenizer(dev_edited_headlines_1, dev_edited_headlines_2, \n",
        "                            max_length = max_h_len, padding = 'max_length',\n",
        "                            truncation = True, return_tensors =\"pt\")\n",
        "\n",
        "###test set\n",
        "\n",
        "test_set_embedding = tokenizer(test_edited_headlines_1, test_edited_headlines_2, \n",
        "                            max_length = max_h_len, padding = 'max_length',\n",
        "                            truncation = True, return_tensors =\"pt\")\n",
        "\n",
        "### extended train set (incl. funlines)\n",
        "\n",
        "ext_set_embedding = tokenizer(ext_edited_headlines_1, ext_edited_headlines_2, \n",
        "                            max_length = max_h_len, padding = 'max_length',\n",
        "                            truncation = True, return_tensors =\"pt\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78eff565636d4a04ab3547212af44ee5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSDqaQn4r67g"
      },
      "source": [
        "####################### PROVIDED (ADJUSTED) #######################\n",
        "\n",
        "# We create a Dataset so we can create minibatches\n",
        "\n",
        "class Task2Dataset_BERT(Dataset):\n",
        "\n",
        "    def __init__(self, ids, att_mask, token_type_id, labels):\n",
        "        self.x1_train = ids.to(device)\n",
        "        self.x2_train = att_mask.to(device)\n",
        "        self.x3_train = token_type_id.to(device)\n",
        "        self.y_train = labels.to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x1_train[item],self.x2_train[item],self.x3_train[item], self.y_train[item]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AderRuhyMk1u"
      },
      "source": [
        "Below we create Task2Dataset_BERT class instances and DataLoader objects to be fed into the BERT models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5O_WWmwbexw"
      },
      "source": [
        "### train set\n",
        "\n",
        "train_dataset = Task2Dataset_BERT(train_set_embedding['input_ids'], \n",
        "                                  train_set_embedding['attention_mask'], \n",
        "                                  train_set_embedding['token_type_ids'], \n",
        "                                  torch.tensor(train_labels))\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "### dev set\n",
        "\n",
        "dev_dataset = Task2Dataset_BERT(dev_set_embedding['input_ids'], \n",
        "                                  dev_set_embedding['attention_mask'], \n",
        "                                  dev_set_embedding['token_type_ids'], \n",
        "                                  torch.tensor(dev_labels))\n",
        "\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "### test set\n",
        "\n",
        "test_dataset = Task2Dataset_BERT(test_set_embedding['input_ids'], \n",
        "                                  test_set_embedding['attention_mask'], \n",
        "                                  test_set_embedding['token_type_ids'], \n",
        "                                  torch.tensor(test_labels))\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "### extended train set\n",
        "\n",
        "ext_dataset = Task2Dataset_BERT(ext_set_embedding['input_ids'], \n",
        "                                ext_set_embedding['attention_mask'], \n",
        "                                ext_set_embedding['token_type_ids'], \n",
        "                                torch.tensor(ext_labels))\n",
        "\n",
        "ext_dataloader = DataLoader(ext_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XVrwmAGPQGe"
      },
      "source": [
        "The following are the functions provided (with slight adjustments) for model training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17OzqCLobg2f"
      },
      "source": [
        "####################### PROVIDED #######################\n",
        "\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, \n",
        "    i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    correct_answers = (output == target)\n",
        "    correct = sum(correct_answers)\n",
        "    acc = np.true_divide(correct,len(output))\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| Acc: {acc:.2f} ')\n",
        "\n",
        "    return correct, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOwyyTjnUBv7"
      },
      "source": [
        "####################### PROVIDED (ADJUSTED) #######################\n",
        "\n",
        "# Adjustments are mainly the commented out lines \n",
        "# (these are left in for the marker's convenience)\n",
        "\n",
        "def eval(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            id, att_mask, token_type_id, labels = batch\n",
        "\n",
        "            #feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            # for RNN:\n",
        "            #model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + labels.shape[0]\n",
        "            #model.hidden = model.init_hidden()\n",
        "            out = model(id, attention_mask = att_mask,\n",
        "                                    token_type_ids = token_type_id,\n",
        "                                    labels = labels)\n",
        "            loss = out[0]\n",
        "            preds = out[1]\n",
        "\n",
        "            # We get the mse\n",
        "            \n",
        "            correct, __ = model_performance(\n",
        "                np.argmax(preds.detach().cpu().numpy(), axis=1), \n",
        "                labels.cpu().numpy())\n",
        "\n",
        "            epoch_loss += loss.item()*labels.shape[0]\n",
        "            epoch_correct += correct\n",
        "            pred_all.extend(preds.detach())\n",
        "            trg_all.extend(labels.detach())\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_correct/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVW4cAA9bkgk"
      },
      "source": [
        "####################### PROVIDED (ADJUSTED) #######################\n",
        "\n",
        "# Similarly to the eval function, djustments are mainly the commented out lines \n",
        "# (& these are left in for the marker's convenience)\n",
        "\n",
        "def train(train_iter, model, number_epoch, optimizer, scheduler, dev_iter = None):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "    print(\"Training model.\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        epoch_correct = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "\n",
        "        for batch in train_iter:\n",
        "            id, att_mask, token_type_id, labels = batch\n",
        "\n",
        "            # for RNN:\n",
        "            #model.batch_size = target.shape[0]\n",
        "            #model.hidden = model.init_hidden()\n",
        "            no_observations = no_observations + labels.shape[0]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(id, attention_mask = att_mask,\n",
        "                                    token_type_ids = token_type_id,\n",
        "                                    labels = labels)\n",
        "            \n",
        "            loss = out[0]\n",
        "            preds = out[1]\n",
        "\n",
        "            correct, __ = model_performance(\n",
        "                np.argmax(preds.detach().cpu().numpy(), axis=1), \n",
        "                labels.cpu().numpy())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()*labels.shape[0]\n",
        "            epoch_correct += correct\n",
        "\n",
        "        valid_loss, valid_acc, __, __ = eval(dev_iter, model)\n",
        "\n",
        "        epoch_loss, epoch_acc = epoch_loss / no_observations, epoch_correct / no_observations        \n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train Accuracy: {epoch_acc:.2f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. Accuracy: {valid_acc:.2f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmN2KjgFzQTq"
      },
      "source": [
        "#### BERT Version 1: BertForSequenceClassification() using the base training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdqGoracbmnB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "959d7bf9ba684e60a28386e0cef4aa67",
            "15dd2b6c2dc9487a8db956f452b09eca",
            "42ab2f07c2b24277bae1bbc480fd9f70",
            "d470c55363cd47c08978504c1a5d7e38",
            "ac6c57094aa74a0b87096d687fd9e6ea",
            "13c41af5b1c24c23afa8e1ca3cfe9bb5",
            "b02f95372366490db735caabeb963455",
            "f32933a7475b4a6fba425da4112b027b",
            "a08183136d0b422a84586cae4bb5079e",
            "5db217c95dce4a8d800712f8744586e4",
            "7138d66a0b6b4205b448bac62e1eb308",
            "a2cbbbb69ddb43559cbbcde35b2c86bb",
            "26c7a4ad490848d0bedd960daf507ca6",
            "3f07b59b16a74008912259901526f276",
            "cf1ea88fe6db47ed93c15f01f78607de",
            "d145ba4906414780855875e2b8c85fab"
          ]
        },
        "outputId": "12b7ad68-88ae-48d4-db4f-b34e0a52fed7"
      },
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels = 3,   \n",
        "                                                      output_attentions = False,\n",
        "                                                      output_hidden_states = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "959d7bf9ba684e60a28386e0cef4aa67",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a08183136d0b422a84586cae4bb5079e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfLS0sIQbomP"
      },
      "source": [
        "# Hyperparameters for BERT v.1:\n",
        "\n",
        "decay = 1e-2\n",
        "lr = 1e-05\n",
        "steps = len(train_dataloader) * epochs\n",
        "wu = 0.06\n",
        "wu_steps = int(steps * wu)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': decay},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = wu_steps,\n",
        "                                            num_training_steps = steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqaEXuFocoRv"
      },
      "source": [
        "train(train_dataloader, model, epochs,optimizer, scheduler, dev_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zuh-w4IV0Y2T",
        "outputId": "57312a85-9e3a-4e7c-9d02-e1b2fc004a15"
      },
      "source": [
        "# Evaluation on test set\n",
        "\n",
        "test_loss, test_acc, __, __ = eval(test_dataloader, model)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.46587837837837837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBApoO9UphP2"
      },
      "source": [
        "#### BERT Version 2: BertForSequenceClassification() using the base training data, but removing data with label 0\n",
        "\n",
        "Note: This has not been used in the report for consistency reasons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHWTkPF7fGLe"
      },
      "source": [
        "# Train the same model but this time without the zero label data\n",
        "\n",
        "# train set\n",
        "\n",
        "h_to_edit_1, h_to_edit_2, train_labels = get_orig_headl_and_new_word_tuples(train_df)\n",
        "\n",
        "filtered_train_data = [(h_1, h_2, label) for (h_1, h_2, label) in zip(h_to_edit_1, h_to_edit_2, train_labels) if label != 0 ]\n",
        "\n",
        "filt_h_to_edit_1 = [tup[0] for tup in filtered_train_data]\n",
        "filt_h_to_edit_2 = [tup[1] for tup in filtered_train_data]\n",
        "filt_labels = [tup[2]-1 for tup in filtered_train_data]\n",
        "\n",
        "filt_edited_headlines_1 = get_edited_headlines(filt_h_to_edit_1)\n",
        "filt_edited_headlines_2 = get_edited_headlines(filt_h_to_edit_2)\n",
        "\n",
        "filt_train_set_embedding = tokenizer(filt_edited_headlines_1, filt_edited_headlines_2, \n",
        "                            max_length = max_h_len, padding = 'max_length',\n",
        "                            truncation = True, return_tensors =\"pt\")\n",
        "\n",
        "\n",
        "filt_train_dataset = Task2Dataset_BERT(filt_train_set_embedding['input_ids'], \n",
        "                                  filt_train_set_embedding['attention_mask'], \n",
        "                                  filt_train_set_embedding['token_type_ids'], \n",
        "                                  torch.tensor(filt_labels))\n",
        "\n",
        "\n",
        "filt_train_dataloader = DataLoader(filt_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# dev set\n",
        "\n",
        "dev_h_to_edit_1, dev_h_to_edit_2, dev_labels = get_orig_headl_and_new_word_tuples(dev_df)\n",
        "\n",
        "filtered_dev_data = [(h_1, h_2, label) for (h_1, h_2, label) in zip(dev_h_to_edit_1, dev_h_to_edit_2, dev_labels) if label != 0 ]\n",
        "\n",
        "filt_dev_h_to_edit_1 = [tup[0] for tup in filtered_dev_data]\n",
        "filt_dev_h_to_edit_2 = [tup[1] for tup in filtered_dev_data]\n",
        "filt_dev_labels = [tup[2]-1 for tup in filtered_dev_data]\n",
        "\n",
        "filt_dev_edited_headlines_1 = get_edited_headlines(filt_dev_h_to_edit_1)\n",
        "filt_dev_edited_headlines_2 = get_edited_headlines(filt_dev_h_to_edit_2)\n",
        "\n",
        "filt_dev_set_embedding = tokenizer(filt_dev_edited_headlines_1, filt_dev_edited_headlines_2, \n",
        "                            max_length = max_h_len, padding = 'max_length',\n",
        "                            truncation = True, return_tensors =\"pt\")\n",
        "\n",
        "\n",
        "filt_dev_dataset = Task2Dataset_BERT(filt_dev_set_embedding['input_ids'], \n",
        "                                  filt_dev_set_embedding['attention_mask'], \n",
        "                                  filt_dev_set_embedding['token_type_ids'], \n",
        "                                  torch.tensor(filt_dev_labels))\n",
        "\n",
        "\n",
        "filt_dev_dataloader = DataLoader(filt_dev_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# test set\n",
        "\n",
        "test_h_to_edit_1, test_h_to_edit_2, test_labels = get_orig_headl_and_new_word_tuples(test_df)\n",
        "\n",
        "filtered_test_data = [(h_1, h_2, label) for (h_1, h_2, label) in zip(test_h_to_edit_1, test_h_to_edit_2, test_labels) if label != 0 ]\n",
        "\n",
        "filt_test_h_to_edit_1 = [tup[0] for tup in filtered_test_data]\n",
        "filt_test_h_to_edit_2 = [tup[1] for tup in filtered_test_data]\n",
        "filt_test_labels = [tup[2]-1 for tup in filtered_test_data]\n",
        "\n",
        "filt_test_edited_headlines_1 = get_edited_headlines(filt_test_h_to_edit_1)\n",
        "filt_test_edited_headlines_2 = get_edited_headlines(filt_test_h_to_edit_2)\n",
        "\n",
        "filt_test_set_embedding = tokenizer(filt_test_edited_headlines_1, filt_test_edited_headlines_2, \n",
        "                            max_length = max_h_len, padding = 'max_length',\n",
        "                            truncation = True, return_tensors =\"pt\")\n",
        "\n",
        "\n",
        "filt_test_dataset = Task2Dataset_BERT(filt_test_set_embedding['input_ids'], \n",
        "                                  filt_test_set_embedding['attention_mask'], \n",
        "                                  filt_test_set_embedding['token_type_ids'], \n",
        "                                  torch.tensor(filt_test_labels))\n",
        "\n",
        "\n",
        "filt_test_dataloader = DataLoader(filt_test_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rti25__MlYiR"
      },
      "source": [
        "model_filt = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                        num_labels = 2,   \n",
        "                                                        output_attentions = False,\n",
        "                                                        output_hidden_states = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr_-G-SNsZnx"
      },
      "source": [
        "# Hyperparameters for BERT v.2:\n",
        "\n",
        "epochs = 4\n",
        "decay = 1e-2\n",
        "lr = 1e-05\n",
        "steps = len(train_dataloader) * epochs\n",
        "wu = 0.06\n",
        "wu_steps = int(steps * wu)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model_filt.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': decay},\n",
        "    {'params': [p for n, p in model_filt.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = wu_steps,\n",
        "                                            num_training_steps = steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAS69sJWlbi0",
        "outputId": "2e220f46-ce8e-438c-e2cb-76a120e26cb8"
      },
      "source": [
        "train(filt_train_dataloader, model_filt, epochs,optimizer, scheduler, filt_dev_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.71 | Train Accuracy: 0.51 |         Val. Loss: 0.69 | Val. Accuracy: 0.51 |\n",
            "| Epoch: 02 | Train Loss: 0.68 | Train Accuracy: 0.55 |         Val. Loss: 0.71 | Val. Accuracy: 0.51 |\n",
            "| Epoch: 03 | Train Loss: 0.62 | Train Accuracy: 0.66 |         Val. Loss: 0.73 | Val. Accuracy: 0.55 |\n",
            "| Epoch: 04 | Train Loss: 0.53 | Train Accuracy: 0.75 |         Val. Loss: 0.76 | Val. Accuracy: 0.56 |\n",
            "| Epoch: 05 | Train Loss: 0.46 | Train Accuracy: 0.79 |         Val. Loss: 0.82 | Val. Accuracy: 0.56 |\n",
            "| Epoch: 06 | Train Loss: 0.42 | Train Accuracy: 0.82 |         Val. Loss: 0.84 | Val. Accuracy: 0.57 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR9aFiy1twcn",
        "outputId": "af650c0d-26fa-409e-be20-b07fa0dad93c"
      },
      "source": [
        "# Evaluation on test set\n",
        "\n",
        "test_loss, test_acc, __, __ = eval(filt_test_dataloader, model_filt)\n",
        "\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5555555555555556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12ChvwaspnIk"
      },
      "source": [
        "#### BERT Version 3: BertForSequenceClassification() using the extended training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goRQvLRXZj66"
      },
      "source": [
        "# Hyperparameter search for BERT v.2:\n",
        "\n",
        "decays = [1e-2, 1e-3]\n",
        "lrs = [1e-05, 1e-04]\n",
        "steps = len(train_dataloader) * epochs\n",
        "wus = [0.06, 0.03]\n",
        "wu_steps = int(steps * wu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi5H1Ywry3hX",
        "outputId": "301cc2c7-2825-46b0-bbee-17a531393686"
      },
      "source": [
        "i = 1\n",
        "\n",
        "for decay in decays:\n",
        "    for lr in lrs:\n",
        "        for wu in wus:\n",
        "            # message\n",
        "            print(f\"Training model number {i} with lr = {lr}, decay = {decay} and wu = {wu}.\")\n",
        "\n",
        "            # initialize model\n",
        "            model_ext = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                        num_labels = 3,   \n",
        "                                                        output_attentions = False,\n",
        "                                                        output_hidden_states = False)\n",
        "            # initialize remaining hyperparams\n",
        "            no_decay = ['bias', 'LayerNorm.weight']\n",
        "            optimizer_grouped_parameters = [\n",
        "                {'params': [p for n, p in model_ext.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': decay},\n",
        "                {'params': [p for n, p in model_ext.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "            optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                        num_warmup_steps = wu_steps,\n",
        "                                                        num_training_steps = steps)\n",
        "            # train model\n",
        "            train(ext_dataloader, model_ext, epochs,optimizer, scheduler, dev_dataloader)\n",
        "            print(\"\")\n",
        "            i+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model number 1 with lr = 1e-05, decay = 0.01 and wu = 0.06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.99 | Train Accuracy: 0.44 |         Val. Loss: 0.96 | Val. Accuracy: 0.43 |\n",
            "| Epoch: 02 | Train Loss: 0.96 | Train Accuracy: 0.46 |         Val. Loss: 0.96 | Val. Accuracy: 0.47 |\n",
            "| Epoch: 03 | Train Loss: 0.94 | Train Accuracy: 0.50 |         Val. Loss: 0.96 | Val. Accuracy: 0.47 |\n",
            "| Epoch: 04 | Train Loss: 0.93 | Train Accuracy: 0.54 |         Val. Loss: 0.97 | Val. Accuracy: 0.46 |\n",
            "\n",
            "Training model number 2 with lr = 1e-05, decay = 0.01 and wu = 0.03.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.99 | Train Accuracy: 0.44 |         Val. Loss: 0.96 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 02 | Train Loss: 0.95 | Train Accuracy: 0.49 |         Val. Loss: 0.97 | Val. Accuracy: 0.45 |\n",
            "| Epoch: 03 | Train Loss: 0.92 | Train Accuracy: 0.54 |         Val. Loss: 0.98 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 04 | Train Loss: 0.89 | Train Accuracy: 0.59 |         Val. Loss: 0.98 | Val. Accuracy: 0.46 |\n",
            "\n",
            "Training model number 3 with lr = 0.0001, decay = 0.01 and wu = 0.06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.99 | Train Accuracy: 0.43 |         Val. Loss: 0.97 | Val. Accuracy: 0.43 |\n",
            "| Epoch: 02 | Train Loss: 0.96 | Train Accuracy: 0.45 |         Val. Loss: 0.96 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 03 | Train Loss: 0.96 | Train Accuracy: 0.45 |         Val. Loss: 0.96 | Val. Accuracy: 0.43 |\n",
            "| Epoch: 04 | Train Loss: 0.96 | Train Accuracy: 0.45 |         Val. Loss: 0.96 | Val. Accuracy: 0.46 |\n",
            "\n",
            "Training model number 4 with lr = 0.0001, decay = 0.01 and wu = 0.03.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.97 | Train Accuracy: 0.45 |         Val. Loss: 0.96 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 02 | Train Loss: 0.96 | Train Accuracy: 0.46 |         Val. Loss: 0.96 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 03 | Train Loss: 0.90 | Train Accuracy: 0.57 |         Val. Loss: 0.95 | Val. Accuracy: 0.53 |\n",
            "| Epoch: 04 | Train Loss: 0.69 | Train Accuracy: 0.73 |         Val. Loss: 1.07 | Val. Accuracy: 0.52 |\n",
            "\n",
            "Training model number 5 with lr = 1e-05, decay = 0.001 and wu = 0.06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.97 | Train Accuracy: 0.45 |         Val. Loss: 0.96 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 02 | Train Loss: 0.96 | Train Accuracy: 0.46 |         Val. Loss: 0.96 | Val. Accuracy: 0.45 |\n",
            "| Epoch: 03 | Train Loss: 0.94 | Train Accuracy: 0.51 |         Val. Loss: 0.97 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 04 | Train Loss: 0.92 | Train Accuracy: 0.55 |         Val. Loss: 0.97 | Val. Accuracy: 0.46 |\n",
            "\n",
            "Training model number 6 with lr = 1e-05, decay = 0.001 and wu = 0.03.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.98 | Train Accuracy: 0.45 |         Val. Loss: 0.96 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 02 | Train Loss: 0.95 | Train Accuracy: 0.51 |         Val. Loss: 0.96 | Val. Accuracy: 0.49 |\n",
            "| Epoch: 03 | Train Loss: 0.88 | Train Accuracy: 0.60 |         Val. Loss: 0.98 | Val. Accuracy: 0.51 |\n",
            "| Epoch: 04 | Train Loss: 0.84 | Train Accuracy: 0.65 |         Val. Loss: 0.98 | Val. Accuracy: 0.51 |\n",
            "\n",
            "Training model number 7 with lr = 0.0001, decay = 0.001 and wu = 0.06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.97 | Train Accuracy: 0.44 |         Val. Loss: 0.96 | Val. Accuracy: 0.44 |\n",
            "| Epoch: 02 | Train Loss: 0.96 | Train Accuracy: 0.46 |         Val. Loss: 0.96 | Val. Accuracy: 0.43 |\n",
            "| Epoch: 03 | Train Loss: 0.96 | Train Accuracy: 0.45 |         Val. Loss: 0.96 | Val. Accuracy: 0.43 |\n",
            "| Epoch: 04 | Train Loss: 0.96 | Train Accuracy: 0.45 |         Val. Loss: 0.96 | Val. Accuracy: 0.46 |\n",
            "\n",
            "Training model number 8 with lr = 0.0001, decay = 0.001 and wu = 0.03.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.98 | Train Accuracy: 0.45 |         Val. Loss: 0.98 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 02 | Train Loss: 0.97 | Train Accuracy: 0.44 |         Val. Loss: 0.96 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 03 | Train Loss: 0.96 | Train Accuracy: 0.45 |         Val. Loss: 0.96 | Val. Accuracy: 0.44 |\n",
            "| Epoch: 04 | Train Loss: 0.95 | Train Accuracy: 0.49 |         Val. Loss: 0.97 | Val. Accuracy: 0.43 |\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1trZ9mLZloc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe3a7a2-5e39-420a-fc97-16dcb2633f55"
      },
      "source": [
        "train(ext_dataloader, model_ext, epochs,optimizer, scheduler, dev_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.99 | Train Accuracy: 0.45 |         Val. Loss: 0.97 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 02 | Train Loss: 0.95 | Train Accuracy: 0.48 |         Val. Loss: 0.96 | Val. Accuracy: 0.47 |\n",
            "| Epoch: 03 | Train Loss: 0.91 | Train Accuracy: 0.57 |         Val. Loss: 0.96 | Val. Accuracy: 0.51 |\n",
            "| Epoch: 04 | Train Loss: 0.82 | Train Accuracy: 0.65 |         Val. Loss: 1.00 | Val. Accuracy: 0.52 |\n",
            "| Epoch: 05 | Train Loss: 0.76 | Train Accuracy: 0.69 |         Val. Loss: 1.02 | Val. Accuracy: 0.51 |\n",
            "| Epoch: 06 | Train Loss: 0.74 | Train Accuracy: 0.70 |         Val. Loss: 1.02 | Val. Accuracy: 0.51 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX-v3ePwtmeL",
        "outputId": "94b853e9-b788-402d-9d1b-86b0b85bd8d2"
      },
      "source": [
        "# Evaluation on test set\n",
        "\n",
        "test_loss, test_acc, __, __ = eval(test_dataloader, model_ext)\n",
        "\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5155405405405405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dLv2L1npu-W"
      },
      "source": [
        "#### BERT Version 4: BertForSequenceClassification() using the extended training data and including the original headline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBowDNq9p0iG"
      },
      "source": [
        "# GET EMBEDDINGS\n",
        "\n",
        "max_h_len = max(find_max_len(edited_headlines_1, 3), find_max_len(ext_edited_headlines_1,3))\n",
        "\n",
        "#dev set\n",
        "\n",
        "dev_set_embedding_o = tokenizer(dev_edited_headlines_1, dev_edited_headlines_2, orig_dev_headlines, \n",
        "                            max_length = max_h_len, padding = 'max_length',\n",
        "                            truncation = True, return_tensors =\"pt\")\n",
        "\n",
        "#test set\n",
        "\n",
        "test_set_embedding_o = tokenizer(test_edited_headlines_1, test_edited_headlines_2, orig_test_headlines,\n",
        "                            max_length = max_h_len, padding = 'max_length',\n",
        "                            truncation = True, return_tensors =\"pt\")\n",
        "\n",
        "# extended train set (incl. funlines)\n",
        "\n",
        "ext_set_embedding_o = tokenizer(ext_edited_headlines_1, ext_edited_headlines_2, orig_ext_headlines,\n",
        "                            max_length = max_h_len, padding = 'max_length',\n",
        "                            truncation = True, return_tensors =\"pt\")\n",
        "\n",
        "\n",
        "# GET DATA LOADERS\n",
        "\n",
        "# dev set\n",
        "\n",
        "dev_dataset_o = Task2Dataset_BERT(dev_set_embedding_o['input_ids'], \n",
        "                                  dev_set_embedding_o['attention_mask'], \n",
        "                                  dev_set_embedding_o['token_type_ids'], \n",
        "                                  torch.tensor(dev_labels))\n",
        "\n",
        "dev_dataloader_o = DataLoader(dev_dataset_o, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# test set\n",
        "\n",
        "test_dataset_o = Task2Dataset_BERT(test_set_embedding_o['input_ids'], \n",
        "                                  test_set_embedding_o['attention_mask'], \n",
        "                                  test_set_embedding_o['token_type_ids'], \n",
        "                                  torch.tensor(test_labels))\n",
        "\n",
        "test_dataloader_o = DataLoader(test_dataset_o, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# extended train set\n",
        "\n",
        "ext_dataset_o = Task2Dataset_BERT(ext_set_embedding_o['input_ids'], \n",
        "                                ext_set_embedding_o['attention_mask'], \n",
        "                                ext_set_embedding_o['token_type_ids'], \n",
        "                                torch.tensor(ext_labels))\n",
        "\n",
        "ext_dataloader_o = DataLoader(ext_dataset_o, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JchyrUkIyVlJ"
      },
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "\n",
        "model_ext_o = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                        num_labels = 3,   \n",
        "                                                        output_attentions = False,\n",
        "                                                        output_hidden_states = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA9OyJ8vySo3"
      },
      "source": [
        "# Hyperparameters for BERT v.4:\n",
        "\n",
        "decay = 1e-2\n",
        "lr = 1e-05\n",
        "steps = len(train_dataloader) * epochs\n",
        "wu = 0.06\n",
        "wu_steps = int(steps * wu)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model_ext_o.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': decay},\n",
        "    {'params': [p for n, p in model_ext_o.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = wu_steps,\n",
        "                                            num_training_steps = steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDbD12nNyZVZ",
        "outputId": "fccd3297-f012-4db4-a3bc-05f0bff52b48"
      },
      "source": [
        "train(ext_dataloader_o, model_ext_o, epochs,optimizer, scheduler, dev_dataloader_o)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.98 | Train Accuracy: 0.46 |         Val. Loss: 0.96 | Val. Accuracy: 0.45 |\n",
            "| Epoch: 02 | Train Loss: 0.95 | Train Accuracy: 0.49 |         Val. Loss: 0.97 | Val. Accuracy: 0.45 |\n",
            "| Epoch: 03 | Train Loss: 0.91 | Train Accuracy: 0.56 |         Val. Loss: 0.98 | Val. Accuracy: 0.47 |\n",
            "| Epoch: 04 | Train Loss: 0.83 | Train Accuracy: 0.64 |         Val. Loss: 1.02 | Val. Accuracy: 0.48 |\n",
            "| Epoch: 05 | Train Loss: 0.78 | Train Accuracy: 0.67 |         Val. Loss: 1.05 | Val. Accuracy: 0.48 |\n",
            "| Epoch: 06 | Train Loss: 0.76 | Train Accuracy: 0.69 |         Val. Loss: 1.05 | Val. Accuracy: 0.48 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZgtFNq3ykSd",
        "outputId": "11a9a840-99cb-48a6-dae0-13179ab942da"
      },
      "source": [
        "# Evaluation on test set\n",
        "\n",
        "test_loss, test_acc, __, __ = eval(test_dataloader_o, model_ext_o)\n",
        "\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.46452702702702703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6eqY5syp3uE"
      },
      "source": [
        "#### BERT Version 5: Use word pairs only for the prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6WVxvjOp-NQ"
      },
      "source": [
        "def get_new_words(data):\n",
        "    \"\"\"\n",
        "    Instead of a list of headlines, this function only extracts the edit words.\n",
        "    \"\"\"\n",
        "\n",
        "    edit_words_1 = [new_word_1 for (original_1, new_word_1) \\\n",
        "                           in zip(data.original1.to_list(), data.edit1.to_list())]\n",
        "\n",
        "    edit_words_2 = [new_word_2 for (original_2, new_word_2) \\\n",
        "                           in zip(data.original2.to_list(), data.edit2.to_list())]\n",
        "\n",
        "    labels = data.label.to_list()\n",
        "    return edit_words_1, edit_words_2, labels\n",
        "\n",
        "train_words_1, train_words_2, train_labels = get_new_words(train_extra_df)\n",
        "dev_words_1, dev_words_2, dev_labels = get_new_words(dev_df)\n",
        "test_words_1, test_words_2, test_labels = get_new_words(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM7hYybGSMkq"
      },
      "source": [
        "# Tokenize\n",
        "\n",
        "### train set incl. funlines\n",
        "\n",
        "train_words_embedding = tokenizer(train_words_1, train_words_2, max_length = 4, \n",
        "                                  padding = 'max_length',truncation = True, \n",
        "                                  return_tensors =\"pt\")\n",
        "###dev set\n",
        "\n",
        "dev_words_embedding = tokenizer(dev_words_1, dev_words_2, max_length = 4, \n",
        "                                padding = 'max_length', truncation = True, \n",
        "                                return_tensors =\"pt\")\n",
        " ###test set\n",
        "\n",
        "test_words_embedding = tokenizer(test_words_1, test_words_2, max_length = 4, \n",
        "                                 padding = 'max_length', truncation = True, \n",
        "                                 return_tensors =\"pt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e7PfhzSSM7H"
      },
      "source": [
        "### train set incl. funlines\n",
        "\n",
        "train_dataset_w = Task2Dataset_BERT(train_words_embedding['input_ids'], \n",
        "                                  train_words_embedding['attention_mask'], \n",
        "                                  train_words_embedding['token_type_ids'], \n",
        "                                  torch.tensor(train_labels))\n",
        "\n",
        "train_dataloader_w = DataLoader(train_dataset_w, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "### dev set\n",
        "\n",
        "dev_dataset_w = Task2Dataset_BERT(dev_words_embedding['input_ids'], \n",
        "                                  dev_words_embedding['attention_mask'], \n",
        "                                  dev_words_embedding['token_type_ids'], \n",
        "                                  torch.tensor(dev_labels))\n",
        "\n",
        "dev_dataloader_w = DataLoader(dev_dataset_w, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "### test set\n",
        "\n",
        "test_dataset_w = Task2Dataset_BERT(test_words_embedding['input_ids'], \n",
        "                                  test_words_embedding['attention_mask'], \n",
        "                                  test_words_embedding['token_type_ids'], \n",
        "                                  torch.tensor(test_labels))\n",
        "\n",
        "test_dataloader_w = DataLoader(test_dataset_w, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOIP022LUHqv",
        "outputId": "b94f73a7-a643-40c8-80ee-3a1d3e5070f8"
      },
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "\n",
        "model_w = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels = 3,   \n",
        "                                                      output_attentions = False,\n",
        "                                                      output_hidden_states = False)\n",
        "\n",
        "# Hyperparameters for BERT v.5:\n",
        "\n",
        "decay = 1e-2\n",
        "lr = 1e-05\n",
        "steps = len(train_dataloader) * epochs\n",
        "wu = 0.06\n",
        "wu_steps = int(steps * wu)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model_w.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': decay},\n",
        "    {'params': [p for n, p in model_w.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = wu_steps,\n",
        "                                            num_training_steps = steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqOqUlAzUEv9",
        "outputId": "a14d326d-22c0-439e-b1b4-5283063d880a"
      },
      "source": [
        "# run 1\n",
        "train(train_dataloader_w, model_w, epochs,optimizer, scheduler, dev_dataloader_w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 1.04 | Train Accuracy: 0.45 |         Val. Loss: 0.97 | Val. Accuracy: 0.46 |\n",
            "| Epoch: 02 | Train Loss: 0.96 | Train Accuracy: 0.45 |         Val. Loss: 0.96 | Val. Accuracy: 0.48 |\n",
            "| Epoch: 03 | Train Loss: 0.95 | Train Accuracy: 0.47 |         Val. Loss: 0.96 | Val. Accuracy: 0.47 |\n",
            "| Epoch: 04 | Train Loss: 0.95 | Train Accuracy: 0.49 |         Val. Loss: 0.96 | Val. Accuracy: 0.50 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-hIxnneUSIM",
        "outputId": "5d587ca7-ffd3-4f3c-d541-0c91ab95f503"
      },
      "source": [
        "# Evaluation on test set\n",
        "\n",
        "test_loss, test_acc, __, __ = eval(test_dataloader_w, model_w)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47635135135135137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_r_DTbjb3Wb"
      },
      "source": [
        "#### BERT Version 6: Use word pairs & original headlines for the prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G33P9meRcCO0"
      },
      "source": [
        "max_len = find_max_len(orig_ext_headlines,1)+10\n",
        "\n",
        "# Tokenize\n",
        "\n",
        "### train set incl. funlines\n",
        "\n",
        "train_words_embedding_o = tokenizer(train_words_1, train_words_2, orig_ext_headlines,\n",
        "                                  max_length = max_len, padding = 'max_length',\n",
        "                                  truncation = True, return_tensors =\"pt\")\n",
        "###dev set\n",
        "\n",
        "dev_words_embedding_o = tokenizer(dev_words_1, dev_words_2, orig_dev_headlines,\n",
        "                                max_length = max_len, padding = 'max_length', \n",
        "                                truncation = True, return_tensors =\"pt\")\n",
        " ###test set\n",
        "\n",
        "test_words_embedding_o = tokenizer(test_words_1, test_words_2, orig_test_headlines,\n",
        "                                 max_length = max_len,  padding = 'max_length', \n",
        "                                 truncation = True, return_tensors =\"pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWHFssq_ehX3"
      },
      "source": [
        "### train set incl. funlines\n",
        "\n",
        "train_dataset_wo = Task2Dataset_BERT(train_words_embedding_o['input_ids'], \n",
        "                                  train_words_embedding_o['attention_mask'], \n",
        "                                  train_words_embedding_o['token_type_ids'], \n",
        "                                  torch.tensor(train_labels))\n",
        "\n",
        "train_dataloader_wo = DataLoader(train_dataset_wo, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "### dev set\n",
        "\n",
        "dev_dataset_wo = Task2Dataset_BERT(dev_words_embedding_o['input_ids'], \n",
        "                                  dev_words_embedding_o['attention_mask'], \n",
        "                                  dev_words_embedding_o['token_type_ids'], \n",
        "                                  torch.tensor(dev_labels))\n",
        "\n",
        "dev_dataloader_wo = DataLoader(dev_dataset_wo, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "### test set\n",
        "\n",
        "test_dataset_wo = Task2Dataset_BERT(test_words_embedding_o['input_ids'], \n",
        "                                  test_words_embedding_o['attention_mask'], \n",
        "                                  test_words_embedding_o['token_type_ids'], \n",
        "                                  torch.tensor(test_labels))\n",
        "\n",
        "test_dataloader_wo = DataLoader(test_dataset_wo, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqTqdZjXfnYT"
      },
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "\n",
        "model_wo = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels = 3,   \n",
        "                                                      output_attentions = False,\n",
        "                                                      output_hidden_states = False)\n",
        "\n",
        "# Hyperparameters for BERT v.6:\n",
        "\n",
        "decay = 1e-2\n",
        "lr = 1e-05\n",
        "steps = len(train_dataloader) * epochs\n",
        "wu = 0.06\n",
        "wu_steps = int(steps * wu)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model_wo.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': decay},\n",
        "    {'params': [p for n, p in model_wo.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = wu_steps,\n",
        "                                            num_training_steps = steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpQULSbzfqDA",
        "outputId": "a90e7d53-9f86-42ab-ac50-841429ddbd5d"
      },
      "source": [
        "# run 1\n",
        "train(train_dataloader_wo, model_wo, epochs,optimizer, scheduler, dev_dataloader_wo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 1.11 | Train Accuracy: 0.40 |         Val. Loss: 0.97 | Val. Accuracy: 0.45 |\n",
            "| Epoch: 02 | Train Loss: 0.95 | Train Accuracy: 0.49 |         Val. Loss: 0.96 | Val. Accuracy: 0.48 |\n",
            "| Epoch: 03 | Train Loss: 0.95 | Train Accuracy: 0.51 |         Val. Loss: 0.96 | Val. Accuracy: 0.50 |\n",
            "| Epoch: 04 | Train Loss: 0.91 | Train Accuracy: 0.56 |         Val. Loss: 0.98 | Val. Accuracy: 0.49 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhvuvVTOft5g",
        "outputId": "1c55f0b8-bd8f-4878-9e36-e8a082080077"
      },
      "source": [
        "# Evaluation on test set\n",
        "\n",
        "test_loss, test_acc, __, __ = eval(test_dataloader_wo, model_wo)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.46824324324324323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q3QMOAHgQ9V",
        "outputId": "d5161bae-8b26-46de-8916-4862d00b1c2b"
      },
      "source": [
        "# run 2\n",
        "train(train_dataloader_wo, model_wo, epochs,optimizer, scheduler, dev_dataloader_wo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 1.08 | Train Accuracy: 0.36 |         Val. Loss: 0.97 | Val. Accuracy: 0.44 |\n",
            "| Epoch: 02 | Train Loss: 0.96 | Train Accuracy: 0.47 |         Val. Loss: 0.96 | Val. Accuracy: 0.44 |\n",
            "| Epoch: 03 | Train Loss: 0.96 | Train Accuracy: 0.47 |         Val. Loss: 0.96 | Val. Accuracy: 0.47 |\n",
            "| Epoch: 04 | Train Loss: 0.94 | Train Accuracy: 0.50 |         Val. Loss: 0.97 | Val. Accuracy: 0.48 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eDcoVQjhUbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e573d31c-76a2-43b2-a83b-a5c9bf42ebb3"
      },
      "source": [
        "# Evaluation on test set\n",
        "\n",
        "test_loss, test_acc, __, __ = eval(test_dataloader_wo, model_wo)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.46756756756756757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLMOOdb4PtMR"
      },
      "source": [
        "# Approach 2 - No Pre-Trained Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaYAlMs2653q"
      },
      "source": [
        "\n",
        "#### FFNN for word funniness score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jrs_YHPO7Ito"
      },
      "source": [
        "# Data formatted for the need of this part\n",
        "# Proportion of training data for train compared to dev\n",
        "train_proportion = 0.8\n",
        "\n",
        "\n",
        "\n",
        "training_data = train_df[['edit1','edit2']]\n",
        "testing_data = test_df[['edit1','edit2']]\n",
        "training_y = train_df[['meanGrade1','meanGrade2']]\n",
        "testing_y = test_df[['meanGrade1','meanGrade2']]\n",
        "training_labels = train_df['label']\n",
        "testing_labels = test_df['label']\n",
        "\n",
        "training_data, dev_data, training_y, dev_y, labels, dev_labels = train_test_split(training_data, training_y, training_labels,\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "# define training and validation data\n",
        "training_data = training_data.to_numpy().reshape(1, 2*len(training_data))[0]\n",
        "training_y = training_y.to_numpy().reshape(1, 2*len(training_y))[0]\n",
        "training_labels = labels.to_numpy()\n",
        "\n",
        "# define validation sets\n",
        "valid_data = dev_data.to_numpy().reshape(1, 2*len(dev_data))[0]\n",
        "valid_y = dev_y.to_numpy().reshape(1, 2*len(dev_y))[0]\n",
        "valid_labels = dev_labels.to_numpy()\n",
        "\n",
        "# define test data\n",
        "testing_data = testing_data.to_numpy().reshape(1, 2*len(testing_data))[0]\n",
        "testing_y = testing_y.to_numpy().reshape(1, 2*len(testing_y))[0]\n",
        "testing_labels = testing_labels.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17QvdeQaGy3L"
      },
      "source": [
        "# Preprocessing data\n",
        "def get_tokenized_corpus(corpus):\n",
        "  tokenized_corpus = []\n",
        "  for sentence in corpus:\n",
        "    tokenized_sentence = []\n",
        "    for token in sentence.split(' '): \n",
        "      tokenized_sentence.append(token)\n",
        "    tokenized_corpus.append(tokenized_sentence)\n",
        " \n",
        "  return tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w2kN2OLG55v"
      },
      "source": [
        "#  method that returns a word to index dictionary\n",
        "def get_word2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  \n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  # we reserve the 0 index for the padding token\n",
        "  word2idx['<pad>'] = 0\n",
        "  \n",
        " \n",
        "  return word2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItL_YEwGHL9S",
        "outputId": "d3796b34-1bbe-43f1-9348-e92d6e3aeb04"
      },
      "source": [
        "def get_model_inputs(tokenized_corpus, word2idx, labels):\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "\n",
        "  # Sentence lengths\n",
        "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
        "\n",
        "  # Get maximum length\n",
        "  max_len = max(sent_lengths)\n",
        "  \n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
        "\n",
        "  # we fill it with our vectorized sentences \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
        "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
        "\n",
        "  # Label tensor\n",
        "  label_tensor = torch.FloatTensor(labels)\n",
        "  return sent_tensor, label_tensor\n",
        "\n",
        "###\n",
        "\n",
        "tokenized_corpus =training_data\n",
        "word2idx = get_word2idx(tokenized_corpus)\n",
        "\n",
        "\n",
        "train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, training_y)\n",
        "\n",
        "print(f'Vocabulary size: {len(word2idx)}')\n",
        "print('Training set tensor:')\n",
        "print(train_sent_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 53\n",
            "Training set tensor:\n",
            "torch.Size([15008, 17])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfA-RIADHVKT"
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):  \n",
        "        super(FFNN, self).__init__()\n",
        "        \n",
        "        # embedding (lookup layer) layer\n",
        "        # padding_idx argument makes sure that the 0-th token in the vocabulary\n",
        "        # is used for padding purposes i.e. its embedding will be a 0-vector\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "    \n",
        "        \n",
        "        # hidden layer\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "    \n",
        "     \n",
        "        # activation\n",
        "        self.relu = nn.LeakyReLU(0.05)\n",
        "\n",
        "        # output layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)  \n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x has shape (batch_size, max_sent_len)\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "        # `embedding` has shape (batch size, max_sent_len, embedding dim)\n",
        "\n",
        "        ########################################################################\n",
        "        # Q: Compute the average embeddings of shape (batch_size, embedding_dim)\n",
        "        ########################################################################\n",
        "        # Implement averaging that ignores padding (average using actual sentence lengths).\n",
        "        # How this effect the result?\n",
        "        \n",
        "        sent_lens = x.ne(0).sum(1, keepdims=True)\n",
        "\n",
        "        averaged = embedded.sum(1) / sent_lens\n",
        "\n",
        "        out = self.fc1(averaged)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSa89_wJHdQb"
      },
      "source": [
        "def accuracy(output, target, labels):\n",
        "  #get accuracy based on which word has a greater meanGrade\n",
        "  predicted_label1 = np.zeros(int(len(output)/2))\n",
        "  predicted_label1[output[0]> output[1]] += 1\n",
        "  predicted_label2 = np.ones(int(len(output)/2))\n",
        "  predicted_label2[output[0]< output[1]] += 2\n",
        "  predicted_labels = predicted_label1 +predicted_label2  \n",
        "\n",
        "  correct = np.zeros(int(len(output)/2))\n",
        "  \n",
        "  correct[predicted_labels.astype(int) == labels.astype(int)] = 1\n",
        "\n",
        "  acc = correct.mean()\n",
        "\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvnjYdQqHtPB"
      },
      "source": [
        "tokenized_valid_corpus = get_tokenized_corpus(valid_data)\n",
        "valid_sent_tensor, valid_label_tensor = get_model_inputs(tokenized_valid_corpus, word2idx, valid_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyfRdYHwH1ol"
      },
      "source": [
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 3\n",
        "\n",
        "# Learning rate \n",
        "LRATE = 0.01\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 20\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 20\n",
        "\n",
        "# the output dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Input and label tensors for training\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "# Input and label tensors for validation\n",
        "feature_valid =  valid_sent_tensor\n",
        "target_valid = valid_label_tensor\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  \n",
        "  # we zero the gradients as they are not removed automatically\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "  # and we need to remove the dimension of size 1\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "\n",
        "  # Compute training accuracy\n",
        "  train_acc = accuracy(predictions, target_train, training_labels)\n",
        "\n",
        "  # calculate the gradient of each parameter\n",
        "  loss.backward()\n",
        "\n",
        "  # update the parameters using the gradients and optimizer algorithm \n",
        "  optimizer.step()\n",
        "  \n",
        "  # this puts the model in \"evaluation mode\" (turns off dropout and batch normalization)\n",
        "  model.eval()\n",
        "\n",
        "  # we do not compute gradients within this block, i.e. no training\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    predictions_valid = torch.where(predictions_valid.isnan(), torch.zeros(predictions_valid.shape), predictions_valid)\n",
        "    torch.set_printoptions(edgeitems=100)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "    valid_acc = accuracy(predictions_valid, target_valid, valid_labels)\n",
        "  \n",
        "  #print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fhtriSxH4NM",
        "outputId": "caba7270-932e-411c-b58b-1ae12b8b1cec"
      },
      "source": [
        "tokenized_test_corpus = get_tokenized_corpus(testing_data)\n",
        "test_sent_tensor, test_label_tensor = get_model_inputs(tokenized_test_corpus, word2idx, testing_y)\n",
        "test_sent_tensor.shape, test_label_tensor.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5920, 0]), torch.Size([5920]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MyzC7JOH6Oa",
        "outputId": "85db186c-0202-4cd7-a4fc-9a242129986a"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "\n",
        "with torch.no_grad():\n",
        "  predictions = model(feature_test).squeeze(1)\n",
        "  predictions = torch.where(predictions.isnan(), torch.zeros(predictions.shape), predictions)\n",
        "  test_loss = loss_fn(predictions, target_test).item()\n",
        "  test_acc = accuracy(predictions, target_test, testing_labels)\n",
        "\n",
        "  # Print\n",
        "  print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.206 | Test Acc: 43.51%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrVfXO07QY49"
      },
      "source": [
        "####CBOW "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeP8-505Ru-W",
        "cellView": "code"
      },
      "source": [
        "# To create our vocab (tokenized, without punctuation and all in lower case)\n",
        "def create_vocab(data):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\"\n",
        "    re_punctuation_string = '[\\s,/.\\']'\n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "\n",
        "    for sentence in data:\n",
        "\n",
        "        tokenized_sentence = []\n",
        "\n",
        "        for token in sentence.split(' '): # simplest split is\n",
        "\n",
        "            token = token.lower()\n",
        "            token = re.sub(re_punctuation_string,'', token)\n",
        "            tokenized_sentence.append(token)\n",
        "\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence in tokenized_corpus:\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "            if token not in vocabulary:\n",
        "\n",
        "                if True:\n",
        "                    vocabulary.append(token)\n",
        "\n",
        "    return vocabulary, tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBxn2YPLR9r2"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_correct = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature, target = batch\n",
        "\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            # for RNN:\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            model.hidden = model.init_hidden()\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            correct, __ = model_performance(np.argmax(pred, axis=1), trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_correct += correct\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_correct/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkFGTtSxQV90"
      },
      "source": [
        "def get_orig_headl_and_new_word_tuples(data):\n",
        "    \"\"\"\n",
        "    Takes a pandas data frame as input.\n",
        "    Each sample of the data set contains a headline article, two edited\n",
        "    versions of this article and a label indicating which of the two edits is\n",
        "    funnier.\n",
        "    Selects relevant columns of the input data, one with the original headlines\n",
        "    and one with the new word to be inserted instead of one of its words, and \n",
        "    converts them into lists of tuples.\n",
        "    Returns four lists.\n",
        "        - A list of tuples of the form (original_headline, new_word) for the \n",
        "          first edited headline\n",
        "        - A list of tuples of the same form for the first edited headline\n",
        "        - A list of labels (3 possible values: 0, 1, and 2. 1 and 2 indicate \n",
        "          which of the edits is funnier, 0 is assigned if they received the same\n",
        "          score.)\n",
        "    \"\"\"\n",
        "\n",
        "    headlines_to_edit_1 = [(original_1, new_word_1) for (original_1, new_word_1) \\\n",
        "                           in zip(data.original1.to_list(), data.edit1.to_list())]\n",
        "\n",
        "    headlines_to_edit_2 = [(original_2, new_word_2) for (original_2, new_word_2) \\\n",
        "                           in zip(data.original2.to_list(), data.edit2.to_list())]\n",
        "\n",
        "    \n",
        "    return headlines_to_edit_1, headlines_to_edit_2\n",
        "\n",
        "def get_original_headlines(data):\n",
        "    pattern = \"\\<(.*?)\\/\\>\"\n",
        "    pattern2 = re.compile(r'\\<(.*?)\\/\\>')\n",
        "    original_headlines = []\n",
        "    for sentence in data:\n",
        "      original_word = re.search(pattern, sentence[0]).group(1)\n",
        "      headline = pattern2.sub(original_word, sentence[0])\n",
        "      original_headlines.append(headline)\n",
        "    return original_headlines\n",
        "    \n",
        "\n",
        "def get_edited_headlines(headline_tuples:list)-> list:\n",
        "    \"\"\"\n",
        "    Takes a list of tuples of form (original_headline, new_word) as input.\n",
        "    Returns a list of edited headlines.\n",
        "    \"\"\"\n",
        "    # list of new edited headlines\n",
        "    edited_headlines = []\n",
        "    #print(headline_tuples[0])\n",
        "\n",
        "    # The word to be replaced in each sentence is denoted as follows:\n",
        "    # <word/> to be replaced\n",
        "    pattern = re.compile(r'\\<(.*?)\\/\\>')\n",
        "    pattern2 = \"\\<(.*?)\\/\\>\"\n",
        "    original_word = re.search(pattern2, headline_tuples[0][0]).group(1)\n",
        "    \n",
        "    for original, new_word in headline_tuples:\n",
        "      edited_headline = pattern.sub(new_word, original)\n",
        "      edited_headlines.append(edited_headline)\n",
        "      \n",
        "\n",
        "    return edited_headlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVlLHWyuQlyo"
      },
      "source": [
        "# data needed for this part\n",
        "train_df = train_df\n",
        "train2_df = train_extra_df\n",
        "# this dev file includes the labels, that will be used to get accuracy, but \n",
        "# won't be used at any point during training (following machine learning methodology)\n",
        "blind_df = dev_df\n",
        "# Extra headlines from online data set\n",
        "extra_headlines = extra_headlines\n",
        "\n",
        "# The training data was split into training and validation during hyperparameter\n",
        "#tunning\n",
        "#training_data, validation_data = train_test_split(train_df,\n",
        "                                           #test_size=(1-train_proportion),\n",
        "                                          #random_state=42)\n",
        "\n",
        "# after hyperparameter tuning - use the original headlines from the three files \n",
        "# above for training (the extra headlines are added in a later cell)\n",
        "training_data = pd.concat([train_df, train2_df, blind_df])\n",
        "testing_data = blind_df\n",
        "\n",
        "# to get model performance\n",
        "training_y = training_data['label'] \n",
        "testing_y = blind_df['label']\n",
        "\n",
        "# new words in headlines  (train)\n",
        "new_word1_training_y = list(training_data['edit1'])\n",
        "new_word2_training_y = list(training_data['edit2'])\n",
        "\n",
        "# new words in headlines (blind-test)\n",
        "new_word1_test_y = list(testing_data['edit1'])\n",
        "new_word2_test_y = list(testing_data['edit2'])\n",
        "\n",
        "h_to_edit_1, h_to_edit_2 = get_orig_headl_and_new_word_tuples(training_data)\n",
        "h_to_edit_1_test, h_to_edit_2_test = get_orig_headl_and_new_word_tuples(testing_data)\n",
        "\n",
        "# train \n",
        "edited_headlines_1 = get_edited_headlines(h_to_edit_1)\n",
        "edited_headlines_2 = get_edited_headlines(h_to_edit_2)\n",
        "original_headlines = get_original_headlines(h_to_edit_1)\n",
        "# The input below was used to try to increase our data set (details about this \n",
        "# are in the report) \n",
        "#extra_original_headlines = extra_headlines.Headline.to_list()\n",
        "\n",
        "# test \n",
        "edited_headlines_1_test = get_edited_headlines(h_to_edit_1_test) \n",
        "edited_headlines_2_test = get_edited_headlines(h_to_edit_2_test)\n",
        "original_headlines_test = get_original_headlines(h_to_edit_1_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p-Pzyr3Q7BS"
      },
      "source": [
        "# create vocabulary sets and corpus for trainign and testing\n",
        "vocab, corpus = create_vocab(original_headlines)\n",
        "vocab_test, corpus_test = create_vocab(original_headlines_test)\n",
        "vocab2, corpus2 = create_vocab(edited_headlines_1)\n",
        "vocab2_test, corpus2_test = create_vocab(edited_headlines_1_test)\n",
        "vocab3, corpus3 = create_vocab(edited_headlines_2)\n",
        "vocab3_test, corpus3_test = create_vocab(edited_headlines_2_test)\n",
        "# The input below was used to try to increase our data set (details about this \n",
        "# are in the report) \n",
        "#extra_vocab, extra_corpus = create_vocab(extra_original_headlines)\n",
        "\n",
        "# add all the words into the training vocab (including words into edited headlines and online dataset)\n",
        "training_vocabulary = vocab + vocab_test + vocab2 + vocab2_test + vocab3 + vocab3_test\n",
        "training_vocabulary = set(training_vocabulary)\n",
        "\n",
        "# add the extra headlines into the training corpus\n",
        "#corpus += extra_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6mWTaJuRDEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8babfcde-eefd-467a-f6b9-5793bdbd1184"
      },
      "source": [
        "# CBOW model\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# tried different context sizes\n",
        "context_size = 2\n",
        "embedding_dim = 200\n",
        "\n",
        "training_set = corpus\n",
        "\n",
        "def make_context_vector(context, word_to_idx):\n",
        "    idxs = [word_to_idx[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "vocabulary = training_vocabulary\n",
        "vocabulary_size = len(vocabulary)\n",
        "print(vocabulary_size)\n",
        "\n",
        "word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
        "\n",
        "data = []\n",
        "\n",
        "for sentence in corpus:\n",
        "    for i in range(2, len(sentence) - 2):\n",
        "        context = [sentence[i-2], sentence[i-1],\n",
        "               sentence[i+1], sentence[i+2]]\n",
        "        target = sentence[i]\n",
        "        data.append((context, target))\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocabulary_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocabulary_size, embedding_dim)\n",
        "        self.proj = nn.Linear(embedding_dim, 128)\n",
        "        self.output = nn.Linear(128, vocabulary_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        embeds = sum(self.embeddings(inputs)).view(1, -1)\n",
        "        out = F.relu(self.proj(embeds))\n",
        "        out = self.output(out)\n",
        "        nll_prob = F.log_softmax(out, dim=-1)\n",
        "        return nll_prob\n",
        "\n",
        "model = CBOW(vocabulary_size, embedding_dim)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "for epoch in range(6):\n",
        "    total_loss = 0\n",
        "    print(epoch)\n",
        "    for context, target in data:\n",
        "        context_vector = make_context_vector(context, word_to_idx)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        nll_prob = model(context_vector)\n",
        "        loss = loss_function(nll_prob, Variable(torch.tensor([word_to_idx[target]])))\n",
        "        \n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "        # update the parameters\n",
        "        optimizer.step() \n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        \n",
        "    losses.append(total_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14891\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEmLdKsSRHew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72438656-35d0-470e-8b25-b9c42ea47614"
      },
      "source": [
        "####### get predictions and run evaluation #############\n",
        "\n",
        "# for edited sentences, need to also do all lower case, punctuation removal and lemmatisation\n",
        "predicted_training = []\n",
        "predicted_test = []\n",
        "re_punctuation_string = '[\\s,/.\\']'\n",
        "for i in range(len(edited_headlines_1)):\n",
        "  headline = edited_headlines_1[i].lower().split(' ')\n",
        "  tokenized_headline = []\n",
        "  for word in headline:\n",
        "    word = re.sub(re_punctuation_string,'', word)\n",
        "    tokenized_headline.append(word)\n",
        "  # for each of the edited headlines, create the context vector around the edited word\n",
        "  #1st edited headline\n",
        "  new_word1_training_y[i] = new_word1_training_y[i].lower()\n",
        "  idx = tokenized_headline.index(new_word1_training_y[i])\n",
        "  # check if edited word is at the begining or end of the sentence, to make \n",
        "  # context smaller than 4 words (given window size = 2)\n",
        "  if idx == 0:\n",
        "    context1 = [tokenized_headline[idx+1], tokenized_headline[idx+2]]\n",
        "  elif idx == 1:\n",
        "    context1 = [ tokenized_headline[idx-1],\n",
        "             tokenized_headline[idx+1], tokenized_headline[idx+2]]\n",
        "  elif idx == len(tokenized_headline) - 1:\n",
        "    context1 = [tokenized_headline[idx-2], tokenized_headline[idx-1]]\n",
        "  elif idx == len(tokenized_headline) - 2:\n",
        "    context1 = [tokenized_headline[idx-2], tokenized_headline[idx-1],\n",
        "             tokenized_headline[idx+1]]\n",
        "  else:\n",
        "    context1 = [tokenized_headline[idx-2], tokenized_headline[idx-1],\n",
        "             tokenized_headline[idx+1], tokenized_headline[idx+2]]\n",
        "\n",
        "  context_vector1 = make_context_vector(context1, word_to_idx)\n",
        "  # 2nd edited headline\n",
        "  headline2 = edited_headlines_2[i].lower().split(' ')\n",
        "  tokenized_headline2 = []\n",
        "  for word in headline2:\n",
        "    word = re.sub(re_punctuation_string,'', word)\n",
        "    tokenized_headline2.append(word)\n",
        "  new_word2_training_y[i] = new_word2_training_y[i].lower()\n",
        "  idx = tokenized_headline2.index(new_word2_training_y[i])\n",
        "  if idx == 0:\n",
        "    context2 = [tokenized_headline2[idx+1], tokenized_headline2[idx+2]]\n",
        "  elif idx == 1:\n",
        "    context2 = [ tokenized_headline2[idx-1],\n",
        "             tokenized_headline2[idx+1], tokenized_headline2[idx+2]]\n",
        "  elif idx == len(tokenized_headline2) - 1:\n",
        "    context2 = [tokenized_headline2[idx-2], tokenized_headline2[idx-1]]\n",
        "  elif idx == len(tokenized_headline2) - 2:\n",
        "    context2 = [tokenized_headline2[idx-2], tokenized_headline2[idx-1],\n",
        "             tokenized_headline2[idx+1]]\n",
        "  else:\n",
        "    context2 = [tokenized_headline2[idx-2], tokenized_headline2[idx-1],\n",
        "             tokenized_headline2[idx+1], tokenized_headline2[idx+2]]         \n",
        "  context_vector2 = make_context_vector(context2, word_to_idx)\n",
        "\n",
        "  #get the model's predictions for given context\n",
        "  \n",
        "  prediction1 = model(context_vector1)\n",
        "  prediction2 = model(context_vector2)\n",
        "\n",
        "  #get the loss between the model's prediction for the given context and the\n",
        "  # word present in the edited headline \n",
        "  loss1 = loss_function(prediction1, Variable(torch.tensor([word_to_idx[new_word1_training_y[i]]])))\n",
        "  loss2 = loss_function(prediction2, Variable(torch.tensor([word_to_idx[new_word2_training_y[i]]])))\n",
        "  \n",
        "  # append label prediction to list\n",
        "  if loss1 > loss2:\n",
        "    predicted_training.append(1)\n",
        "  else:\n",
        "    predicted_training.append(2)\n",
        "  \n",
        "  ###############################################################\n",
        "  #repeat everything for the testing set\n",
        "  ###############################################################\n",
        "\n",
        "  # for each of the edited headlines, create the context vector around the edited word\n",
        "  #1st edited headline\n",
        "for i in range(len(edited_headlines_1_test)):\n",
        "  headline = edited_headlines_1_test[i].lower().split(' ')\n",
        "  tokenized_headline = []\n",
        "  for word in headline:\n",
        "    word = re.sub(re_punctuation_string,'', word)\n",
        "    tokenized_headline.append(word)\n",
        "  new_word1_test_y[i] = new_word1_test_y[i].lower()\n",
        "  idx = tokenized_headline.index(new_word1_test_y[i])\n",
        "  if idx == 0:\n",
        "    context1 = [tokenized_headline[idx+1], tokenized_headline[idx+2]]\n",
        "  elif idx == 1:\n",
        "    context1 = [ tokenized_headline[idx-1],\n",
        "             tokenized_headline[idx+1], tokenized_headline[idx+2]]\n",
        "  elif idx == len(tokenized_headline) - 1:\n",
        "    context1 = [tokenized_headline[idx-2], tokenized_headline[idx-1]]\n",
        "  elif idx == len(tokenized_headline) - 2:\n",
        "    context1 = [tokenized_headline[idx-2], tokenized_headline[idx-1],\n",
        "             tokenized_headline[idx+1]]\n",
        "  else:\n",
        "    context1 = [tokenized_headline[idx-2], tokenized_headline[idx-1],\n",
        "             tokenized_headline[idx+1], tokenized_headline[idx+2]]\n",
        "  context_vector1 = make_context_vector(context1, word_to_idx)\n",
        "  # 2nd edited headline\n",
        "  headline2 = edited_headlines_2_test[i].lower().split(' ')\n",
        "  tokenized_headline2 = []\n",
        "  for word in headline2:\n",
        "    word = re.sub(re_punctuation_string,'', word)\n",
        "    tokenized_headline2.append(word) \n",
        "  new_word2_test_y[i] = new_word2_test_y[i].lower()\n",
        "  idx = tokenized_headline2.index(new_word2_test_y[i])\n",
        "  if idx == 0:\n",
        "    context2 = [tokenized_headline2[idx+1], tokenized_headline2[idx+2]]\n",
        "  elif idx == 1:\n",
        "    context2 = [tokenized_headline2[idx-1],\n",
        "             tokenized_headline2[idx+1], tokenized_headline2[idx+2]]\n",
        "  elif idx == len(tokenized_headline2) - 1:\n",
        "    context2 = [tokenized_headline2[idx-2], tokenized_headline2[idx-1]]\n",
        "  elif idx == len(tokenized_headline2) - 2:\n",
        "    context2 = [tokenized_headline2[idx-2], tokenized_headline2[idx-1],\n",
        "             tokenized_headline2[idx+1]]\n",
        "  else:\n",
        "    context2 = [tokenized_headline2[idx-2], tokenized_headline2[idx-1],\n",
        "             tokenized_headline2[idx+1], tokenized_headline2[idx+2]]\n",
        "  context_vector2 = make_context_vector(context2, word_to_idx)\n",
        "\n",
        "  #get the model's predictions for given context\n",
        "  predicton1 = model(context_vector1)\n",
        "  prediction2 = model(context_vector2)\n",
        "\n",
        "  #get the loss between the model's prediction for the given context and the\n",
        "  # word present in the edited headline \n",
        "  loss1 = loss_function(prediction1, Variable(torch.tensor([word_to_idx[new_word1_test_y[i]]])))\n",
        "  loss2 = loss_function(prediction2, Variable(torch.tensor([word_to_idx[new_word2_test_y[i]]])))\n",
        "  \n",
        "  if loss1 > loss2:\n",
        "    predicted_test.append(1)\n",
        "  else:\n",
        "    predicted_test.append(2)\n",
        "\n",
        "  \n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_training, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted_test, testing_y, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train performance:\n",
            "| Acc: 0.49 \n",
            "\n",
            "Dev performance:\n",
            "| Acc: 0.48 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
